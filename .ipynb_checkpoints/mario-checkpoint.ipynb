{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-super-mario-bros==7.3.0 opencv-python\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "import gym_super_mario_bros\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        resize_obs = transform.resize(observation, self.shape)\n",
    "        # cast float back to uint8\n",
    "        resize_obs *= 255\n",
    "        resize_obs = resize_obs.astype(np.uint8)\n",
    "        return resize_obs\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        pass\n",
    "    \n",
    "class Mario:\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "    self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "    if self.use_cuda:\n",
    "      self.net = self.net.to(device='cuda')\n",
    "\n",
    "    self.exploration_rate = 1\n",
    "    self.exploration_rate_decay = 0.99999975\n",
    "    self.exploration_rate_min = 0.1\n",
    "    self.curr_step = 0\n",
    "\n",
    "    self.save_every = 5e5\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    # EXPLORE\n",
    "    if np.random.rand() < self.exploration_rate:\n",
    "        action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "    # EXPLOIT\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        state = state.unsqueeze(0)\n",
    "        action_values = self.net(state, model='online')\n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "    # decrease exploration_rate\n",
    "    self.exploration_rate *= self.exploration_rate_decay\n",
    "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "    # increment step\n",
    "    self.curr_step += 1\n",
    "    return action_idx\n",
    "\n",
    "class Mario(Mario): # subclassing for continuity\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.memory = deque(maxlen=100000)\n",
    "    self.batch_size = 32\n",
    "\n",
    "\n",
    "  def cache(self, state, next_state, action, reward, done):\n",
    "    state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "    next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "    action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "    reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "    done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "    self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "  def recall(self):\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "class MarioNet(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      c, h, w = input_dim\n",
    "\n",
    "      if h != 84:\n",
    "          raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "      if w != 84:\n",
    "          raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "      self.online = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(3136, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, output_dim)\n",
    "      )\n",
    "\n",
    "      self.target = copy.deepcopy(self.online)\n",
    "\n",
    "      for p in self.target.parameters():\n",
    "          p.requires_grad = False\n",
    "\n",
    "  def forward(self, input, model):\n",
    "      if model == 'online':\n",
    "          return self.online(input)\n",
    "      elif model == 'target':\n",
    "          return self.target(input)\n",
    "\n",
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.gamma = 0.9\n",
    "\n",
    "  def td_estimate(self, state, action):\n",
    "    current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "    return current_Q\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def td_target(self, reward, next_state, done):\n",
    "    next_state_Q = self.net(next_state, model='online')\n",
    "    best_action = torch.argmax(next_state_Q, axis=1)\n",
    "    next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "      super().__init__(state_dim, action_dim, save_dir)\n",
    "      self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "      self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "      loss = self.loss_fn(td_estimate, td_target)\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "      self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e5  # min. experiences before training\n",
    "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4   # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "      if self.curr_step % self.sync_every == 0:\n",
    "          self.sync_Q_target()\n",
    "\n",
    "      if self.curr_step % self.save_every == 0:\n",
    "          self.save()\n",
    "\n",
    "      if self.curr_step < self.burnin:\n",
    "          return None, None\n",
    "\n",
    "      if self.curr_step % self.learn_every != 0:\n",
    "          return None, None\n",
    "\n",
    "      # Sample from memory\n",
    "      state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "      # Get TD Estimate\n",
    "      td_est = self.td_estimate(state, action)\n",
    "\n",
    "      # Get TD Target\n",
    "      td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "      # Backpropagate loss through Q_online\n",
    "      loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "      return (td_est.mean().item(), loss)\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = mario.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (0.19.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (2.8)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (2.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (9.1.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (1.8.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (2022.4.22)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (1.22.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\14752\\anaconda3\\envs\\mario1\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ee55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31926d8cc0059276b147661eddd17ee4369df73ed9fcc20d15b2cbe5340b87db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
